{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4T6QHHOnfcQ"
      },
      "source": [
        "# Part 1: Build CpG Detector\n",
        "\n",
        "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
        "\n",
        "We have defined a few helper functions / parameters for performing this task.\n",
        "\n",
        "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
        "\n",
        "A good solution will be a model that can be trained, with high confidence in correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "mfS4cLmZD2oB"
      },
      "outputs": [],
      "source": [
        "from typing import Sequence\n",
        "from functools import partial\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_f-brPAvKvTn"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE HERE\n",
        "def set_seed(seed=13):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(13)\n",
        "\n",
        "# Use this for getting x label\n",
        "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
        "    for i in range(n_seqs):\n",
        "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
        "\n",
        "# Use this for getting y label\n",
        "def count_cpgs(seq: str) -> int:\n",
        "    cgs = 0\n",
        "    for i in range(0, len(seq) - 1):\n",
        "        dimer = seq[i:i+2]\n",
        "        # note that seq is a string, not a list\n",
        "        if dimer == \"CG\":\n",
        "            cgs += 1\n",
        "    return cgs\n",
        "\n",
        "# Alphabet helpers\n",
        "alphabet = 'NACGT'\n",
        "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
        "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
        "\n",
        "intseq_to_dnaseq = partial(map, int2dna.get)\n",
        "dnaseq_to_intseq = partial(map, dna2int.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK9Qg5GHYxOb"
      },
      "outputs": [],
      "source": [
        "# we prepared two datasets for training and evaluation\n",
        "# training data scale we set to 2048\n",
        "# we test on 512\n",
        "\n",
        "def prepare_data(num_samples=100):\n",
        "    # prepared the training and test data\n",
        "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
        "    # step 1\n",
        "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
        "    \"\"\"\n",
        "    hint:\n",
        "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X\n",
        "        2. You first convert ids back to DNA sequence\n",
        "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
        "    \"\"\"\n",
        "    #step2\n",
        "    temp =  [list(intseq_to_dnaseq(i)) for i in X_dna_seqs_train] # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
        "    #step3\n",
        "    y_dna_seqs =  [count_cpgs(\"\".join(i)) for i in temp] # use count_cpgs here to generate labels with temp generated in step2\n",
        "\n",
        "    return X_dna_seqs_train, y_dna_seqs\n",
        "\n",
        "train_x, train_y = prepare_data(2048)\n",
        "test_x, test_y = prepare_data(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smbw0wqp43ff"
      },
      "outputs": [],
      "source": [
        "# some config\n",
        "LSTM_HIDDEN = 128\n",
        "LSTM_LAYER = 2\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epoch_num = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KWeiQt343fg"
      },
      "outputs": [],
      "source": [
        "# create data loader\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def convert_to_tensor(data, dtype=torch.long):\n",
        "    return torch.tensor(data, dtype=dtype)\n",
        "\n",
        "train_x_tensor = convert_to_tensor(train_x)\n",
        "train_y_tensor = convert_to_tensor(train_y)\n",
        "test_x_tensor = convert_to_tensor(test_x)\n",
        "test_y_tensor = convert_to_tensor(test_y)\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8fgxrM0LnLy"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class CpGPredictor(torch.nn.Module):\n",
        "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0.5):\n",
        "        super(CpGPredictor, self).__init__()\n",
        "        # TODO complete model, you are free to add whatever layers you need here\n",
        "        # We do need a lstm and a classifier layer here but you are free to implement them in your way\n",
        "        self.embedding = torch.nn.Embedding(5, input_dim)\n",
        "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
        "        self.classifier = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO complete forward function\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        logits = self.classifier(lstm_out)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFYZEXy_43fj"
      },
      "outputs": [],
      "source": [
        "# init model / loss function / optimizer etc.\n",
        "\n",
        "LSTM_LAYER = 1\n",
        "learning_rate = 0.001\n",
        "EPOCH_NUM = 100\n",
        "input_dim = 128\n",
        "LSTM_HIDDEN = 128\n",
        "layer_dim = 1\n",
        "output_dim = 1\n",
        "\n",
        "model = CpGPredictor(input_dim, LSTM_HIDDEN, layer_dim, output_dim)\n",
        "# loss_fn = torch.nn.MSELoss()\n",
        "loss_fn = torch.nn.SmoothL1Loss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmod_sGQ43fm"
      },
      "outputs": [],
      "source": [
        "# training (you can modify the code below)\n",
        "# t_loss = .0\n",
        "# model.train()\n",
        "# model.zero_grad()\n",
        "# for _ in range(epoch_num):\n",
        "#     for batch in train_data_loader:\n",
        "#         #TODO complete training loop\n",
        "#         t_loss += loss.item()\n",
        "#         loss.backward()\n",
        "\n",
        "#     print(t_loss)\n",
        "#     t_loss = .0\n",
        "\n",
        "def train(model, train_data_loader, loss_fn, EPOCH_NUM=100):\n",
        "  for epoch in range(EPOCH_NUM):\n",
        "    model.train()\n",
        "    for inputs, labels in train_data_loader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = loss_fn(outputs.squeeze(), labels.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epoch_num}, Loss: {loss.item()}')\n",
        "\n",
        "train(model, train_data_loader, loss_fn, EPOCH_NUM=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykOCPtQb43fo",
        "outputId": "549a3e32-2b0f-47bb-adfd-8afc890e04ff",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss on Test Data: 1.20827254652977\n"
          ]
        }
      ],
      "source": [
        "# eval (you can modify the code below)\n",
        "# model.eval()\n",
        "\n",
        "res_gs = []\n",
        "res_pred = []\n",
        "\n",
        "def evaluate_model(model, test_loader, loss_fn):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      outputs = model(inputs)\n",
        "      loss = loss_fn(outputs.squeeze(), labels.float())\n",
        "      total_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  avg_loss = total_loss/len(test_loader)\n",
        "  print(f'Average Loss on Test Data: {avg_loss}')\n",
        "\n",
        "\n",
        "test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "evaluate_model(model, test_loader, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zctwTvMZ43fp"
      },
      "outputs": [],
      "source": [
        "# Testing wth examples\n",
        "\n",
        "examples = [\"NCACANNTNCGGAGGCGNA\", \"NCACANNTNCGGAGGCGCG\"]\n",
        "\n",
        "def get_actual_count(examples):\n",
        "  return [count_cpgs(\"\".join(i)) for i in examples]\n",
        "\n",
        "def encode_to_integer(examples):\n",
        "  int_sequence = [list(dnaseq_to_intseq(i)) for i in examples]\n",
        "  return int_sequence\n",
        "\n",
        "def transform_examples(data):\n",
        "  int_sequence = encode_to_integer(data)\n",
        "  test_sequnce = convert_to_tensor(int_sequence)\n",
        "  return test_sequnce\n",
        "\n",
        "def predict(model, unseen_data):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    inputs = transform_examples(unseen_data)\n",
        "    outputs = model(inputs)\n",
        "    predictions = outputs.squeeze().cpu().numpy()\n",
        "  return predictions\n",
        "\n",
        "predictions = predict(model, examples)\n",
        "actual_vals = get_actual_count(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYg9tYvfJ5td",
        "outputId": "93d31a5c-aad1-4729-96b2-a4b4f4d87dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequence: NCACANNTNCGGAGGCGNA, Predicted CpGs: 6, Actual CpGs: 2\n",
            "Sequence: NCACANNTNCGGAGGCGCG, Predicted CpGs: 6, Actual CpGs: 3\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(examples)):\n",
        "  print(f\"Sequence: {examples[i]}, Predicted CpGs: {round(predictions[i])}, Actual CpGs: {actual_vals[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMrRf_aVDRJm"
      },
      "source": [
        "# Part 2: what if the DNA sequences are not the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6HG0oaG43fr"
      },
      "outputs": [],
      "source": [
        "# hint we will need following imports\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "AKvG-MNuXJr9"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE HERE\n",
        "random.seed(13)\n",
        "\n",
        "# Use this for getting x label\n",
        "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
        "    for i in range(n_seqs):\n",
        "        seq_len = random.randint(lb, ub)\n",
        "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
        "\n",
        "\n",
        "# Use this for getting y label\n",
        "def count_cpgs(seq: str) -> int:\n",
        "    cgs = 0\n",
        "    for i in range(0, len(seq) - 1):\n",
        "        dimer = seq[i:i+2]\n",
        "        # note that seq is a string, not a list\n",
        "        if dimer == \"CG\":\n",
        "            cgs += 1\n",
        "    return cgs\n",
        "\n",
        "\n",
        "# Alphabet helpers\n",
        "alphabet = 'NACGT'\n",
        "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
        "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
        "dna2int.update({\"pad\": 0})\n",
        "int2dna.update({0: \"<pad>\"})\n",
        "\n",
        "intseq_to_dnaseq = partial(map, int2dna.get)\n",
        "dnaseq_to_intseq = partial(map, dna2int.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExIghFlA43fs"
      },
      "outputs": [],
      "source": [
        "# TODO complete the task based on the change\n",
        "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
        "    # TODO prepared the training and test data\n",
        "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
        "    #step 1\n",
        "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
        "    #step 2\n",
        "    temp = [list(intseq_to_dnaseq(i)) for i in X_dna_seqs_train]\n",
        "    #step3\n",
        "    y_dna_seqs = [count_cpgs(\"\".join(i)) for i in temp]\n",
        "\n",
        "    return X_dna_seqs_train, y_dna_seqs\n",
        "\n",
        "\n",
        "min_len, max_len = 64, 128\n",
        "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
        "test_x, test_y = prepare_data(512, min_len, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlaSy-ty43ft"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, lists, labels) -> None:\n",
        "        self.lists = lists\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lists)\n",
        "\n",
        "\n",
        "# this will be a collate_fn for dataloader to pad sequence\n",
        "class PadSequence:\n",
        "  def __call__(self, batch):\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    sequences, labels = zip(*batch)\n",
        "\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=dna2int['pad'])\n",
        "    label_tensor = torch.Tensor(labels).long()\n",
        "\n",
        "    return padded_sequences, label_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "CAU10Zj043ft"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataset = MyDataset(train_x, train_y)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=PadSequence(), shuffle=True)\n",
        "\n",
        "test_dataset = MyDataset(test_x, test_y)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=PadSequence(), shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgQdxuprkrR0"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class CpGPredictor(torch.nn.Module):\n",
        "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob=0.5):\n",
        "        super(CpGPredictor, self).__init__()\n",
        "        # TODO complete model, you are free to add whatever layers you need here\n",
        "        # We do need a lstm and a classifier layer here but you are free to implement them in your way\n",
        "        self.embedding = torch.nn.Embedding(6, input_dim)\n",
        "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(dropout_prob)\n",
        "        self.classifier = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO complete forward function\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        logits = self.classifier(lstm_out)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD1ry4IZfXva",
        "outputId": "581f2015-6e18-4a16-a65d-89992691332e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 1.1707208156585693\n",
            "Epoch 2/100, Loss: 1.3886330127716064\n",
            "Epoch 4/100, Loss: 1.158286452293396\n",
            "Epoch 5/100, Loss: 1.058457612991333\n",
            "Epoch 6/100, Loss: 1.2002017498016357\n",
            "Epoch 7/100, Loss: 1.0855227708816528\n",
            "Epoch 8/100, Loss: 0.9004484415054321\n",
            "Epoch 9/100, Loss: 0.9857519268989563\n",
            "Epoch 10/100, Loss: 1.0864111185073853\n",
            "Epoch 11/100, Loss: 0.8217935562133789\n",
            "Epoch 12/100, Loss: 0.9576296210289001\n",
            "Epoch 13/100, Loss: 0.9363933205604553\n",
            "Epoch 14/100, Loss: 1.0122158527374268\n",
            "Epoch 15/100, Loss: 0.9667893052101135\n",
            "Epoch 16/100, Loss: 0.9405697584152222\n",
            "Epoch 17/100, Loss: 1.0823549032211304\n",
            "Epoch 18/100, Loss: 1.1331709623336792\n",
            "Epoch 19/100, Loss: 1.0601670742034912\n",
            "Epoch 20/100, Loss: 0.8752838373184204\n",
            "Epoch 21/100, Loss: 0.7618842124938965\n",
            "Epoch 22/100, Loss: 0.9221265316009521\n",
            "Epoch 23/100, Loss: 0.7274192571640015\n",
            "Epoch 24/100, Loss: 0.9484416246414185\n",
            "Epoch 25/100, Loss: 0.9782217741012573\n",
            "Epoch 26/100, Loss: 0.8381202816963196\n",
            "Epoch 27/100, Loss: 0.7835568189620972\n",
            "Epoch 28/100, Loss: 0.6800447106361389\n",
            "Epoch 29/100, Loss: 1.0561754703521729\n",
            "Epoch 30/100, Loss: 0.8259083032608032\n",
            "Epoch 31/100, Loss: 0.8968178033828735\n",
            "Epoch 32/100, Loss: 0.9388854503631592\n",
            "Epoch 33/100, Loss: 0.5971348881721497\n",
            "Epoch 34/100, Loss: 0.864372193813324\n",
            "Epoch 35/100, Loss: 0.7268794178962708\n",
            "Epoch 36/100, Loss: 0.6939553022384644\n",
            "Epoch 37/100, Loss: 0.7043138146400452\n",
            "Epoch 38/100, Loss: 0.6058292388916016\n",
            "Epoch 39/100, Loss: 0.7039719223976135\n",
            "Epoch 40/100, Loss: 0.5237895250320435\n",
            "Epoch 41/100, Loss: 0.582858145236969\n",
            "Epoch 42/100, Loss: 0.6859321594238281\n",
            "Epoch 43/100, Loss: 0.39617016911506653\n",
            "Epoch 44/100, Loss: 0.5740411877632141\n",
            "Epoch 45/100, Loss: 0.5582119822502136\n",
            "Epoch 46/100, Loss: 0.5022756457328796\n",
            "Epoch 47/100, Loss: 0.4260863661766052\n",
            "Epoch 48/100, Loss: 0.546618640422821\n",
            "Epoch 49/100, Loss: 0.5420370101928711\n",
            "Epoch 50/100, Loss: 0.5981311798095703\n",
            "Epoch 51/100, Loss: 0.5327374339103699\n",
            "Epoch 52/100, Loss: 0.37396472692489624\n",
            "Epoch 53/100, Loss: 0.6183371543884277\n",
            "Epoch 54/100, Loss: 0.3838875889778137\n",
            "Epoch 55/100, Loss: 0.3571319282054901\n",
            "Epoch 56/100, Loss: 0.5780400037765503\n",
            "Epoch 57/100, Loss: 0.26668021082878113\n",
            "Epoch 58/100, Loss: 0.5906118154525757\n",
            "Epoch 59/100, Loss: 0.39398324489593506\n",
            "Epoch 60/100, Loss: 0.4100796580314636\n",
            "Epoch 61/100, Loss: 0.2549774646759033\n",
            "Epoch 62/100, Loss: 0.3582199215888977\n",
            "Epoch 63/100, Loss: 0.32024168968200684\n",
            "Epoch 64/100, Loss: 0.28258758783340454\n",
            "Epoch 65/100, Loss: 0.5517314672470093\n",
            "Epoch 66/100, Loss: 0.33604684472084045\n",
            "Epoch 67/100, Loss: 0.21751637756824493\n",
            "Epoch 68/100, Loss: 0.31953495740890503\n",
            "Epoch 69/100, Loss: 0.31957992911338806\n",
            "Epoch 70/100, Loss: 0.3991811275482178\n",
            "Epoch 71/100, Loss: 0.4373496472835541\n",
            "Epoch 72/100, Loss: 0.30888432264328003\n",
            "Epoch 73/100, Loss: 0.2646470069885254\n",
            "Epoch 74/100, Loss: 0.35659170150756836\n",
            "Epoch 75/100, Loss: 0.20803511142730713\n",
            "Epoch 76/100, Loss: 0.36220782995224\n",
            "Epoch 77/100, Loss: 0.37247592210769653\n",
            "Epoch 78/100, Loss: 0.26052144169807434\n",
            "Epoch 79/100, Loss: 0.3199959993362427\n",
            "Epoch 80/100, Loss: 0.4025946259498596\n",
            "Epoch 81/100, Loss: 0.31308725476264954\n",
            "Epoch 82/100, Loss: 0.4888732433319092\n",
            "Epoch 83/100, Loss: 0.32267916202545166\n",
            "Epoch 84/100, Loss: 0.4670233130455017\n",
            "Epoch 85/100, Loss: 0.5121886134147644\n",
            "Epoch 86/100, Loss: 0.3377334475517273\n",
            "Epoch 87/100, Loss: 0.2550463378429413\n",
            "Epoch 88/100, Loss: 0.38351503014564514\n",
            "Epoch 89/100, Loss: 0.3007368743419647\n",
            "Epoch 90/100, Loss: 0.32043325901031494\n",
            "Epoch 91/100, Loss: 0.28690221905708313\n",
            "Epoch 92/100, Loss: 0.2879638969898224\n",
            "Epoch 93/100, Loss: 0.49286460876464844\n",
            "Epoch 94/100, Loss: 0.2742585837841034\n",
            "Epoch 95/100, Loss: 0.3704345226287842\n",
            "Epoch 96/100, Loss: 0.5641093850135803\n",
            "Epoch 97/100, Loss: 0.21252912282943726\n",
            "Epoch 98/100, Loss: 0.30930277705192566\n",
            "Epoch 99/100, Loss: 0.31986865401268005\n",
            "Epoch 100/100, Loss: 0.321469247341156\n"
          ]
        }
      ],
      "source": [
        "LSTM_LAYER = 1\n",
        "learning_rate = 0.001\n",
        "EPOCH_NUM = 100\n",
        "input_dim = 256 #128\n",
        "LSTM_HIDDEN = 128\n",
        "layer_dim = 1\n",
        "output_dim = 1\n",
        "\n",
        "model = CpGPredictor(input_dim, LSTM_HIDDEN, layer_dim, output_dim)\n",
        "# loss_fn = torch.nn.MSELoss()\n",
        "loss_fn = torch.nn.SmoothL1Loss()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train(model, train_loader, loss_fn, EPOCH_NUM=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hCkeRHfy5R",
        "outputId": "f9b2668b-1672-4131-ec1d-a5a183cf958d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss on Test Data: 0.2769583389163017\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_loader, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG0fJuk1f55I",
        "outputId": "fb5a339e-e92f-45ee-cab1-78ced9901fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-56f30dbc03ad>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(data, dtype=dtype)\n"
          ]
        }
      ],
      "source": [
        "# Testing wth examples\n",
        "\n",
        "examples = [\"NCACANNTNCGGAGGCGNA\", \"NCACANNTNCGGAGGCGCG\", \"CG\"]\n",
        "\n",
        "def get_actual_count(examples):\n",
        "  return [count_cpgs(\"\".join(i)) for i in examples]\n",
        "\n",
        "def encode_to_integer(examples):\n",
        "  int_sequence = [list(dnaseq_to_intseq(i)) for i in examples]\n",
        "  return int_sequence\n",
        "\n",
        "def transform_examples(data):\n",
        "  int_sequence = encode_to_integer(data)\n",
        "  padded_sequence = pad_sequence([torch.tensor(seq) for seq in int_sequence], batch_first=True, padding_value=dna2int['pad'])\n",
        "  test_sequnce = convert_to_tensor(padded_sequence)\n",
        "  return test_sequnce\n",
        "\n",
        "def predict(model, unseen_data):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    inputs = transform_examples(unseen_data)\n",
        "    outputs = model(inputs)\n",
        "    predictions = outputs.squeeze().cpu().numpy()\n",
        "  return predictions\n",
        "\n",
        "predictions = predict(model, examples)\n",
        "actual_vals = get_actual_count(examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-8yin6ff_yI",
        "outputId": "44819ea4-622c-4069-a760-4f309db32798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: NCACANNTNCGGAGGCGNA, Predicted CpGs: 5, Actual CpGs: 2\n",
            "Sequence: NCACANNTNCGGAGGCGCG, Predicted CpGs: 4, Actual CpGs: 3\n",
            "Sequence: CG, Predicted CpGs: 4, Actual CpGs: 1\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(examples)):\n",
        "  print(f\"Sequence: {examples[i]}, Predicted CpGs: {round(predictions[i])}, Actual CpGs: {actual_vals[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-z8qv11hvqC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}